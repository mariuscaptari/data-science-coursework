{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spliting data into known and unknown labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv('./data/raw_data/data.csv', header=None)\n",
    "labels = pd.read_csv('./data/raw_data/labels.csv', header=None)\n",
    "\n",
    "data_labelled = data[:len(labels.index)]\n",
    "data_unlabelled = data[len(labels.index):]\n",
    "\n",
    "data_labelled.to_csv('./data/processed_data/known_labels.csv')\n",
    "data_unlabelled.to_csv('./data/processed_data/unknown_labels.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transforming target variable into booleans"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "labels = labels.rename(columns={0: 'target'})\n",
    "labels['target'] = labels['target'].map({1: False, 2: True})\n",
    "labels.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0   False\n",
       "1   False\n",
       "2   False\n",
       "3   False\n",
       "4   False"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class ratio and null values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print('Class ratio:\\n', labels['target'].value_counts())\n",
    "print('Columns with null values:\\n', data_labelled.columns[data_labelled.isna().any()].tolist())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class ratio:\n",
      " False    156\n",
      "True      23\n",
      "Name: target, dtype: int64\n",
      "Columns with null values:\n",
      " []\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like theres a high class imbalance however no columns have null values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "X = data_labelled.copy()\n",
    "y = labels.copy()\n",
    "\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X shape:  (179, 186)\n",
      "y shape:  (179, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split train and test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a html page to vizualize feature destribution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# ONLY HAS TO RUN ONCE TO GENERATE THE HMTL FILE\n",
    "# profile = ProfileReport(df, minimal=True)\n",
    "# profile.to_file(\"visualization/output.html\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From looking at all the feature histograms and the exploratory analysis that was done, there are quite a lot of features with wide ranges of variance. Scalling features will be important specially for K-nearest neighbours and other distance sensitive models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Metric"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given that we have an imbalanced dataset recall or precision wouldn't be good enough measures of performance. We decided to go with f1-score given how well it tends to represent the performance of a model, even if it has an umbalanced target."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Trees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets just try out a simple decision tree without much pre-processing to have a baseline performance metric."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Max depth should be one of the main parameters to tune, given that the deeper the tree is, the more likely it is to overfit and perform badly on unseen data. We'll use a **stratified 5 fold cross validation** for all of our experiments."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "baseline_tree_pipeline = Pipeline(\n",
    "    [   \n",
    "        ('smote',SMOTE(random_state=42)),\n",
    "        ('model',DecisionTreeClassifier(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_tree_search = GridSearchCV(\n",
    "    estimator = baseline_tree_pipeline,\n",
    "    param_grid = {\n",
    "        'model__criterion':['gini','entropy'],\n",
    "    },\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "baseline_tree_search.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "print('Best parameters:\\n', baseline_tree_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(baseline_tree_search.best_score_, 3))\n",
    "print('Score in test dataset:\\n', round(baseline_tree_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = baseline_tree_search.best_estimator_.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best parameters:\n",
      " {'model__criterion': 'entropy'}\n",
      "Best score in cross-validation:\n",
      " 0.73\n",
      "Score in test dataset:\n",
      " 0.8\n",
      "Confusion marix:\n",
      " [[30  1]\n",
      " [ 1  4]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ANOVA + SMOTE Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From our exploratory analysis it seems like ANOVA and Relief work the best. Lets start with using ANOVA classification to get the top n features and apply SMOTE to try and combat the class imbalance but creating new instances of the rarest class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "improved_tree_pipeline = Pipeline(\n",
    "    [\n",
    "     ('selector',SelectKBest(f_classif)),\n",
    "     ('smote',SMOTE(random_state=42)),\n",
    "     ('model',DecisionTreeClassifier(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "improved_tree_search = GridSearchCV(\n",
    "    estimator = improved_tree_pipeline,\n",
    "    param_grid = {\n",
    "        'selector__k':[5,10,20,50,70,100],\n",
    "        'model__criterion':['gini','entropy'],\n",
    "        'model__max_depth':[1,2,4,6,10],\n",
    "        'model__min_samples_split':[2,3,4],\n",
    "        'model__min_samples_leaf':[1,2,4],\n",
    "        'model__max_features':['auto','log2','sqrt',None],\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "improved_tree_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "decision_tree_best = improved_tree_search.best_estimator_\n",
    "\n",
    "print('Best parameters:\\n', improved_tree_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(improved_tree_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(improved_tree_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = decision_tree_best.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 2160 candidates, totalling 10800 fits\n",
      "Best parameters:\n",
      " {'model__criterion': 'gini', 'model__max_depth': 4, 'model__max_features': None, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'selector__k': 50}\n",
      "Best score in cross-validation:\n",
      " 0.849\n",
      "Score in test dataset:\n",
      " 0.6\n",
      "Confusion marix:\n",
      " [[29  2]\n",
      " [ 2  3]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems like the decision tree performs best with a max_depth set to 5 while choosing the top 5 features from the ANOVA analysis. Limiting the depth of the tree helps with having better performance since we're running CV and it punishes trees that overfit with a higher max depth."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K-Nearest Neighbors "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just as we did with Decision Trees we'll first create a baseline KNN model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "baseline_knn_pipeline = Pipeline(\n",
    "    [   \n",
    "        ('smote',SMOTE(random_state=42)),\n",
    "        ('model',KNeighborsClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_knn_search = GridSearchCV(\n",
    "    estimator = baseline_knn_pipeline,\n",
    "    param_grid = {\n",
    "    'model__weights':['uniform','distance'],\n",
    "    'model__n_neighbors':[1,3,5,7,9],\n",
    "    'model__metric':['euclidean','manhattan','chebyshev','minkowski']\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "baseline_knn_search.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "print('Best parameters:\\n', baseline_knn_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(baseline_knn_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(baseline_knn_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = baseline_knn_search.best_estimator_.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best parameters:\n",
      " {'model__metric': 'euclidean', 'model__n_neighbors': 9, 'model__weights': 'uniform'}\n",
      "Best score in cross-validation:\n",
      " 0.726\n",
      "Score in test dataset:\n",
      " 0.833\n",
      "Confusion marix:\n",
      " [[29  2]\n",
      " [ 0  5]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We didnt get great results perhaps because KNN relies on using a distance function between features and we didn't use a feauture scaler. We've seen that some our features have quite different ranges in which they can vary. Let's use a feature scaller as the first step on our pipeline followed by SMOTE."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "improved_knn_pipeline = Pipeline(\n",
    "    [('scaller',StandardScaler()),\n",
    "     ('selector',SelectKBest(f_classif)),\n",
    "     ('smote',SMOTE(random_state=42)),\n",
    "     ('model',KNeighborsClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "improved_knn_search = GridSearchCV(\n",
    "    estimator = improved_knn_pipeline,\n",
    "    param_grid = {\n",
    "        'scaller':[StandardScaler(), MinMaxScaler(),Normalizer(), MaxAbsScaler()],\n",
    "        'selector__k':[5,10,20,50,70,100],\n",
    "        'model__weights':['uniform','distance'],\n",
    "        'model__n_neighbors':[1,3,5,7,9],\n",
    "        'model__metric':['euclidean','manhattan','chebyshev','minkowski']\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "improved_knn_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "knn_best = improved_knn_search.best_estimator_\n",
    "\n",
    "print('Best parameters:\\n', improved_knn_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(improved_knn_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(improved_knn_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = knn_best.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 960 candidates, totalling 4800 fits\n",
      "Best parameters:\n",
      " {'model__metric': 'euclidean', 'model__n_neighbors': 1, 'model__weights': 'uniform', 'scaller': StandardScaler(), 'selector__k': 50}\n",
      "Best score in cross-validation:\n",
      " 0.971\n",
      "Score in test dataset:\n",
      " 0.889\n",
      "Confusion marix:\n",
      " [[31  0]\n",
      " [ 1  4]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applying a standart scaller and using feature upsampling made a big difference in the performance of the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "KNN reveals to have better performance than Decision Trees measuring by the f1 score. Interesting to note that here KNN tends to perform better with more features, given that the best results come from using the top 40 in comparison to the top 5 in the decision tree."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ensemble Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our own ensemble classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To create our own ensemble model we'll use the best performing model of each categafory we've tested so far: Decision Trees and KNN."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "estimators=[('decision_tree', make_pipeline(SelectKBest(f_classif, k=50), SMOTE(random_state=42), decision_tree_best)),\n",
    "('knn',make_pipeline(StandardScaler(),SelectKBest(f_classif, k=50), SMOTE(random_state=42), knn_best))]\n",
    "\n",
    "# voting set to hard so that majority wins\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "ensemble.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = ensemble.predict(X_test)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "print('Score in test dataset:\\n', round(f1_score,3))\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score in test dataset:\n",
      " 0.75\n",
      "Confusion marix:\n",
      " [[31  0]\n",
      " [ 2  3]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "random_forest_pipeline = Pipeline(\n",
    "    [('selector',SelectKBest(f_classif)),\n",
    "     ('smote',SMOTE(random_state=42)),\n",
    "     ('model',RandomForestClassifier(random_state=42))]\n",
    ")\n",
    "\n",
    "random_forest_search = GridSearchCV(\n",
    "    estimator = random_forest_pipeline,\n",
    "    param_grid = {\n",
    "        'selector__k':[5,10,20,50,70,100],\n",
    "        'model__criterion':['gini','entropy'],\n",
    "        'model__max_depth':[2,3,5,10],\n",
    "        'model__min_samples_split':[2,3,4],\n",
    "        'model__min_samples_leaf': [1,2,4],\n",
    "        'model__n_estimators':[200]\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "random_forest_search.fit(X_train,y_train.values.ravel())\n",
    "random_forest_best = random_forest_search.best_estimator_\n",
    "\n",
    "print('Best parameters:\\n', random_forest_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(random_forest_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(random_forest_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = random_forest_best.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "Best parameters:\n",
      " {'model__criterion': 'gini', 'model__max_depth': 5, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 800, 'selector__k': 70}\n",
      "Best score in cross-validation:\n",
      " 0.876\n",
      "Score in test dataset:\n",
      " 1.0\n",
      "Confusion marix:\n",
      " [[31  0]\n",
      " [ 0  5]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As expected with random rorests being another ensemble model, it performs very well, specially in the test dataset where it predicted 100% of the targets correctly"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict on the unlaballed dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Decision Tree**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "y_pred_validation = decision_tree_best.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False    153\n",
       "True      27\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**K-Nearest Neighbors**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "y_pred_validation = knn_best.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False    159\n",
       "True      21\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ensemble**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "y_pred_validation = ensemble.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False    164\n",
       "True      16\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Random Forest**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "y_pred_validation = random_forest_best.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False    160\n",
       "True      20\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The search for the best model continues in the next notebook..."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc5ccfa982ac0c806d028cc5c3bc1fe8554eb246e53476722f6c4c66d9ffd506"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}