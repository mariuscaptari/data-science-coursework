{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into known and unknown labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/raw_data/data.csv', header=None)\n",
    "labels = pd.read_csv('./data/raw_data/labels.csv', header=None)\n",
    "\n",
    "data_labelled = data[:len(labels.index)]\n",
    "data_unlabelled = data[len(labels.index):]\n",
    "\n",
    "data_labelled.to_csv('./data/processed_data/known_labels.csv')\n",
    "data_unlabelled.to_csv('./data/processed_data/unknown_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming target variable into booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0   False\n",
       "1   False\n",
       "2   False\n",
       "3   False\n",
       "4   False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels.rename(columns={0: 'target'})\n",
    "labels['target'] = labels['target'].map({1: False, 2: True})\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class ratio and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ratio:\n",
      " False    156\n",
      "True      23\n",
      "Name: target, dtype: int64\n",
      "Columns with null values:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "print('Class ratio:\\n', labels['target'].value_counts())\n",
    "print('Columns with null values:\\n', data_labelled.columns[data_labelled.isna().any()].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like theres a high class imbalance however no columns have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (179, 186)\n",
      "y shape:  (179, 1)\n"
     ]
    }
   ],
   "source": [
    "X = data_labelled.copy()\n",
    "y = labels.copy()\n",
    "\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a html page to vizualize feature destribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY HAS TO RUN ONCE TO GENERATE THE HMTL FILE\n",
    "# profile = ProfileReport(df, minimal=True)\n",
    "# profile.to_file(\"visualization/output.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at all the feature histograms and the exploratory analysis that was done, there are quite a lot of features with wide ranges of variance. Scalling features will be important specially for K-nearest neighbours and other distance sensitive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have an imbalanced dataset recall or precision wouldn't be good enough measures of performance. We decided to go with f1-score given how well it tends to represent the performance of a model, even if it has an umbalanced target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just try out a simple decision tree without much pre-processing to have a baseline performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max depth should be one of the main parameters to tune, given that the deeper the tree is, the more likely it is to overfit and perform badly on unseen data. We'll use a **stratified 5 fold cross validation** for all of our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Best parameters:\n",
      " {'model__criterion': 'entropy'}\n",
      "Best score in cross-validation:\n",
      " 0.73\n",
      "Score in test dataset:\n",
      " 0.8\n",
      "Confusion marix:\n",
      " [[30  1]\n",
      " [ 1  4]]\n"
     ]
    }
   ],
   "source": [
    "baseline_tree_pipeline = Pipeline(\n",
    "    [   \n",
    "        ('smote',SMOTE(random_state=42)),\n",
    "        ('model',DecisionTreeClassifier(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_tree_search = GridSearchCV(\n",
    "    estimator = baseline_tree_pipeline,\n",
    "    param_grid = {\n",
    "        'model__criterion':['gini','entropy'],\n",
    "    },\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "baseline_tree_search.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "print('Best parameters:\\n', baseline_tree_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(baseline_tree_search.best_score_, 3))\n",
    "print('Score in test dataset:\\n', round(baseline_tree_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = baseline_tree_search.best_estimator_.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA + SMOTE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our exploratory analysis it seems like ANOVA and Relief work the best. Lets start with using ANOVA classification to get the top n features and apply SMOTE to try and combat the class imbalance but creating new instances of the rarest class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2160 candidates, totalling 10800 fits\n",
      "Best parameters:\n",
      " {'model__criterion': 'gini', 'model__max_depth': 3, 'model__max_features': None, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'selector__k': 50}\n",
      "Best score in cross-validation:\n",
      " 0.849\n",
      "Score in test dataset:\n",
      " 0.6\n",
      "Confusion marix:\n",
      " [[29  2]\n",
      " [ 2  3]]\n"
     ]
    }
   ],
   "source": [
    "improved_tree_pipeline = Pipeline(\n",
    "    [\n",
    "     ('selector',SelectKBest(f_classif)),\n",
    "     ('smote',SMOTE(random_state=42)),\n",
    "     ('model',DecisionTreeClassifier(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "improved_tree_search = GridSearchCV(\n",
    "    estimator = improved_tree_pipeline,\n",
    "    param_grid = {\n",
    "        'selector__k':[5,10,20,50,70,100],\n",
    "        'model__criterion':['gini','entropy'],\n",
    "        'model__max_depth':[1,2,3,5,None],\n",
    "        'model__min_samples_split':[2,3,4],\n",
    "        'model__min_samples_leaf':[1,2,4],\n",
    "        'model__max_features':['auto','log2','sqrt',None],\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "improved_tree_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "decision_tree_best = improved_tree_search.best_estimator_\n",
    "\n",
    "print('Best parameters:\\n', improved_tree_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(improved_tree_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(improved_tree_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = decision_tree_best.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the depth of the tree helps with having better performance since we're running CV and it punishes trees that overfit with a higher max depth. However, the more tunned tree still seems to overfitting since it performed quite worse on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did with Decision Trees we'll first create a baseline KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "Best parameters:\n",
      " {'model__metric': 'euclidean', 'model__n_neighbors': 17, 'model__weights': 'uniform'}\n",
      "Best score in cross-validation:\n",
      " 0.835\n",
      "Score in test dataset:\n",
      " 0.833\n",
      "Confusion marix:\n",
      " [[29  2]\n",
      " [ 0  5]]\n"
     ]
    }
   ],
   "source": [
    "baseline_knn_pipeline = Pipeline(\n",
    "    [   \n",
    "        ('smote',SMOTE(random_state=42)),\n",
    "        ('model',KNeighborsClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_knn_search = GridSearchCV(\n",
    "    estimator = baseline_knn_pipeline,\n",
    "    param_grid = {\n",
    "    'model__weights':['uniform','distance'],\n",
    "    'model__n_neighbors':range(1,21,2),\n",
    "    'model__metric':['euclidean','manhattan','chebyshev','minkowski']\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "baseline_knn_search.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "print('Best parameters:\\n', baseline_knn_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(baseline_knn_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(baseline_knn_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = baseline_knn_search.best_estimator_.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didnt get great results perhaps because KNN relies on using a distance function between features and we didn't use a feauture scaler. We've seen that some our features have quite different ranges in which they can vary. Let's use a feature scaller as the first step on our pipeline followed by SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n",
      "Best parameters:\n",
      " {'model__metric': 'euclidean', 'model__n_neighbors': 1, 'model__weights': 'uniform', 'selector__k': 50}\n",
      "Best score in cross-validation:\n",
      " 0.971\n",
      "Score in test dataset:\n",
      " 0.889\n",
      "Confusion marix:\n",
      " [[31  0]\n",
      " [ 1  4]]\n"
     ]
    }
   ],
   "source": [
    "improved_knn_pipeline = Pipeline(\n",
    "    [\n",
    "    ('scaller',StandardScaler()),\n",
    "    ('selector',SelectKBest(f_classif)),\n",
    "    ('smote',SMOTE(random_state=42)),\n",
    "    ('model',KNeighborsClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "improved_knn_search = GridSearchCV(\n",
    "    estimator = improved_knn_pipeline,\n",
    "    param_grid = {\n",
    "        'selector__k':[5,10,20,50,70,100],\n",
    "        'model__weights':['uniform','distance'],\n",
    "        'model__n_neighbors':range(1,21,2),\n",
    "        'model__metric':['euclidean','manhattan','chebyshev','minkowski']\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "improved_knn_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "knn_best = improved_knn_search.best_estimator_\n",
    "\n",
    "print('Best parameters:\\n', improved_knn_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(improved_knn_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(improved_knn_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = knn_best.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a standart scaller and using feature upsampling made a big difference in the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN reveals to have better performance than Decision Trees measuring by the f1 score. Interesting to note that here KNN tends to perform better with more features, given that the best results come from using the top 40 in comparison to the top 5 in the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "Best parameters:\n",
      " {'model__degree': 2, 'model__kernel': 'poly', 'selector__k': 50}\n",
      "Best score in cross-validation:\n",
      " 0.914\n",
      "Score in test dataset:\n",
      " 1.0\n",
      "Confusion marix:\n",
      " [[31  0]\n",
      " [ 0  5]]\n"
     ]
    }
   ],
   "source": [
    "svc_pipeline = Pipeline(\n",
    "    [\n",
    "    ('scaller',StandardScaler()),\n",
    "    ('selector',SelectKBest(f_classif)),\n",
    "    ('smote',SMOTE(random_state=42)),\n",
    "    ('model',SVC(random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "svc_search = GridSearchCV(\n",
    "    estimator = svc_pipeline,\n",
    "    param_grid = {\n",
    "        'selector__k':[5,10,20,50,70,80,90,100],\n",
    "        'model__kernel':['linear', 'poly',],\n",
    "        'model__degree':[2,3,4,5]\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "svc_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "svc_best = svc_search.best_estimator_\n",
    "\n",
    "print('Best parameters:\\n', svc_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(svc_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(svc_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = svc_best.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Baynes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters:\n",
      " {'model__var_smoothing': 1e-11, 'selector__k': 70}\n",
      "Best score in cross-validation:\n",
      " 0.871\n",
      "Score in test dataset:\n",
      " 1.0\n",
      "Confusion marix:\n",
      " [[31  0]\n",
      " [ 0  5]]\n"
     ]
    }
   ],
   "source": [
    "baynes_pipeline = Pipeline(\n",
    "    [\n",
    "    ('scaller',StandardScaler()),\n",
    "    ('selector',SelectKBest(f_classif)),\n",
    "    ('smote',SMOTE(random_state=42)),\n",
    "    ('model',GaussianNB())\n",
    "    ]\n",
    ")\n",
    "\n",
    "baynes_search = GridSearchCV(\n",
    "    estimator = baynes_pipeline,\n",
    "    param_grid = {\n",
    "        'selector__k':[5,10,20,50,70,80,90,100],\n",
    "        'model__var_smoothing': [1e-11, 1e-10, 1e-9]\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "baynes_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "baynes_best = baynes_search.best_estimator_\n",
    "\n",
    "print('Best parameters:\\n', baynes_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(baynes_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(baynes_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = baynes_best.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our own ensemble classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our own ensemble model we'll use the best performing models we've tested so far: k-NN, SVC and Baynes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score in test dataset:\n",
      " 0.889\n",
      "Confusion marix:\n",
      " [[31  0]\n",
      " [ 1  4]]\n"
     ]
    }
   ],
   "source": [
    "estimators=[\n",
    "    # ('decision_tree', make_pipeline(SelectKBest(f_classif, k=50), SMOTE(random_state=42), decision_tree_best)),\n",
    "    ('knn',make_pipeline(StandardScaler(),SelectKBest(f_classif, k=50), SMOTE(random_state=42), knn_best)),\n",
    "    ('svc',make_pipeline(StandardScaler(),SelectKBest(f_classif, k=100), SMOTE(random_state=42), svc_best)),\n",
    "    ('baynes',make_pipeline(StandardScaler(),SelectKBest(f_classif, k=70), SMOTE(random_state=42), baynes_best)),\n",
    "]\n",
    "\n",
    "# voting set to hard so that majority wins\n",
    "ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "ensemble.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = ensemble.predict(X_test)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "print('Score in test dataset:\\n', round(f1_score,3))\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n",
      "Best parameters:\n",
      " {'model__criterion': 'entropy', 'model__max_depth': 5, 'model__min_samples_leaf': 1, 'model__min_samples_split': 4, 'model__n_estimators': 100, 'selector__k': 70}\n",
      "Best score in cross-validation:\n",
      " 0.886\n",
      "Score in test dataset:\n",
      " 1.0\n",
      "Confusion marix:\n",
      " [[31  0]\n",
      " [ 0  5]]\n"
     ]
    }
   ],
   "source": [
    "random_forest_pipeline = Pipeline(\n",
    "    [('selector',SelectKBest(f_classif)),\n",
    "     ('smote',SMOTE(random_state=42)),\n",
    "     ('model',RandomForestClassifier(random_state=42))]\n",
    ")\n",
    "\n",
    "random_forest_search = GridSearchCV(\n",
    "    estimator = random_forest_pipeline,\n",
    "    param_grid = {\n",
    "        'selector__k':[70],\n",
    "        'model__criterion':['gini','entropy'],\n",
    "        'model__max_depth':[2,3,4,5,6],\n",
    "        'model__min_samples_split':[2,3,4],\n",
    "        'model__min_samples_leaf': [1,2,3,4],\n",
    "        'model__n_estimators':[100,200,500]\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")\n",
    "random_forest_search.fit(X_train,y_train.values.ravel())\n",
    "random_forest_best = random_forest_search.best_estimator_\n",
    "\n",
    "print('Best parameters:\\n', random_forest_search.best_params_)\n",
    "print('Best score in cross-validation:\\n', round(random_forest_search.best_score_,3))\n",
    "print('Score in test dataset:\\n', round(random_forest_search.score(X_test, y_test),3))\n",
    "\n",
    "y_pred = random_forest_best.predict(X_test)\n",
    "print('Confusion marix:\\n', confusion_matrix(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected with random forests being another ensemble model, it performs very well, specially in the test dataset where it predicted 100% of the targets correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on the unlaballed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    155\n",
       "True      25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_validation = decision_tree_best.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    159\n",
       "True      21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_validation = knn_best.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Baynes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    155\n",
       "True      25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_validation = baynes_best.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    163\n",
       "True      17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_validation = svc_best.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    161\n",
       "True      19\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_validation = ensemble.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    159\n",
       "True      21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_validation = random_forest_best.predict(data_unlabelled)\n",
    "pd.DataFrame(y_pred_validation).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The search for the best model continues in the next notebook**..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e78ec37bc2882541b38fbf35ccf4cc81f8c74cc3ebbf3055180d00090407bc4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
