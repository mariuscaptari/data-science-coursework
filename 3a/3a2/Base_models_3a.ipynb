{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spliting data into known and unknown labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv('./data/raw_data/data.csv', header=None)\n",
    "labels = pd.read_csv('./data/raw_data/labels.csv', header=None)\n",
    "\n",
    "data_labelled = data[:len(labels.index)]\n",
    "data_unlabelled = data[len(labels.index):]\n",
    "\n",
    "data_labelled.to_csv('./data/processed_data/known_labels.csv')\n",
    "data_unlabelled.to_csv('./data/processed_data/unknown_labels.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Merge known labels dataframe with the respective labels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "labels = labels.rename(columns={0: 'y'})\n",
    "df = pd.concat([data_labelled, labels], axis=1)\n",
    "\n",
    "df['y'] = df['y'].map({1: False, 2: True})\n",
    "\n",
    "df.to_csv('./data/processed_data/data_with_labels.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Class ratio and null values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print('Class ratio:\\n', labels['y'].value_counts())\n",
    "print('Columns with null values:\\n', df.columns[df.isna().any()].tolist())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Class ratio:\n",
      " 1    156\n",
      "2     23\n",
      "Name: y, dtype: int64\n",
      "Columns with null values:\n",
      " []\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems like theres a high class imbalance however no columns have null values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a vizualization html page for our dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# profile = ProfileReport(df, minimal=True)\n",
    "# profile.to_file(\"visualization/output.html\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Trees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From our exploratory analysis it seems like ANOVA and Relief work the best. Lets start with using ANOVA classification to get the top features and apply SMOTE to try and combat the class imbalance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build a pipeline with the ANOVA feature selector > SMOTE > Decision Tree Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "tree_pipeline = Pipeline(\n",
    "    [\n",
    "     ('selector',SelectKBest(f_classif)),\n",
    "     ('smote',SMOTE(random_state=42)),\n",
    "     ('model',DecisionTreeClassifier(random_state=42))\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grid Search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the parameters that we'll want to test during our 5 fold cross validation. Lets use F1 score for measuring the performance of our model given the class imbalance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "tree_search = GridSearchCV(\n",
    "    estimator = tree_pipeline,\n",
    "    param_grid = {\n",
    "        'selector__k':[5,10,15,20,30,40,50],\n",
    "        'model__criterion':['gini','entropy'],\n",
    "        'model__max_depth':range(1,20),\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Restricting max_depth should help with preventing overfitting along with a 5 fold cross validation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "tree_search.fit(data_labelled,labels.values.ravel())\n",
    "\n",
    "#y_pred = search.best_estimator_.predict(data_labelled)\n",
    "\n",
    "print('Best parameters:\\n', tree_search.best_params_)\n",
    "print('Best score f1:\\n',tree_search.best_score_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 266 candidates, totalling 1330 fits\n",
      "Best parameters:\n",
      " {'model__criterion': 'entropy', 'model__max_depth': 5, 'selector__k': 5}\n",
      "Best score f1:\n",
      " 0.9811492673992674\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems like the decision tree performs best with a max_depth set to 5 while choosing the top 5 features from the ANOVA analysis. Limiting the depth of the tree helps with having better performance since we're running CV and it punishes trees that overfit with a higher max depth."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Nearest Neighbors "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given that KNN relies on majority voting based on class membership of k nearest neighbors, we should use a feature scaller as the first step on our pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "knn_pipeline = Pipeline(\n",
    "    [('scaller',StandardScaler()),\n",
    "     ('selector',SelectKBest(f_classif)),\n",
    "     ('smote',SMOTE(random_state=42)),\n",
    "     ('model',KNeighborsClassifier())\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "knn_search = GridSearchCV(\n",
    "    estimator = knn_pipeline,\n",
    "    param_grid = {'selector__k':[5,10,15,20,30,40,50],\n",
    "    'model__weights':['uniform','distance'],\n",
    "    'model__n_neighbors':[1,3,5,7,9],\n",
    "    'model__metric':['euclidean','manhattan','chebyshev','minkowski']\n",
    "    },\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    verbose=1\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "knn_search.fit(data_labelled,labels.values.ravel())\n",
    "\n",
    "print('Best parameters:\\n', knn_search.best_params_)\n",
    "print('Best score f1:\\n',knn_search.best_score_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 280 candidates, totalling 1400 fits\n",
      "Best parameters:\n",
      " {'model__metric': 'chebyshev', 'model__n_neighbors': 7, 'model__weights': 'uniform', 'selector__k': 40}\n",
      "Best score f1:\n",
      " 0.9937484737484737\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "KNN reveals to have better performance than Decision Trees measuring by the f1 score. K equal to 7 is the most optimal number of neighbors to consider while using the chebyshev distance with an uniform weight. Interesting to note that here KNN tends to perform better with more features, given that the best results come from using the top 40 in comparison to the top 5 in the decision tree."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "cc5ccfa982ac0c806d028cc5c3bc1fe8554eb246e53476722f6c4c66d9ffd506"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}